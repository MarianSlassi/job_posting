{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3ad853",
   "metadata": {},
   "source": [
    "### INIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71007779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dir for import: C:\\Users\\Мариан\\Desktop\\Jupyter Notes\\Projects\\Trainee_iFortex\\Git\\job_posting\n",
      "Config initialized\n"
     ]
    }
   ],
   "source": [
    "# System/env config\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "parent_dir = Path.cwd().resolve().parent\n",
    "sys.path.append(str(parent_dir))\n",
    "print('Current dir for import:', parent_dir)\n",
    "\n",
    "from src.config import Config\n",
    "config = Config()\n",
    "print('Config initialized')\n",
    "\n",
    "\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Modules for data \n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, EarlyStoppingCallback, AutoModelForTokenClassification, DataCollatorForTokenClassification\n",
    ")\n",
    "\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import CRF\n",
    "from seqeval.metrics import f1_score as f1_span, precision_score as p_span, recall_score as r_span, classification_report\n",
    "from seqeval.scheme import IOB2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cdc34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds = load_dataset(\"jjzha/skillspan\", cache_dir=config['raw_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0af079ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'tokens', 'tags_skill', 'tags_knowledge', 'source'],\n",
       "        num_rows: 4800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'tokens', 'tags_skill', 'tags_knowledge', 'source'],\n",
       "        num_rows: 3174\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'tokens', 'tags_skill', 'tags_knowledge', 'source'],\n",
       "        num_rows: 3569\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127327c5",
   "metadata": {},
   "source": [
    "### Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bfc3635",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds['train'].select_columns(['tokens', 'tags_skill']).to_pandas()\n",
    "df_validation = ds['validation'].select_columns(['tokens', 'tags_skill']).to_pandas()\n",
    "df_test = ds['test'].select_columns(['tokens', 'tags_skill']).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dae98ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags_skill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Senior, QA, Engineer, (, m/f/d, ), &lt;ORGANIZAT...</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[&lt;ADDRESS&gt;, &lt;ADDRESS&gt;, &lt;ADDRESS&gt;, &lt;ADDRESS&gt;, &lt;...</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Date, posted:, 2021-07-14]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Likes:, 0, Dislikes:, 0, Love:, 0]</td>\n",
       "      <td>[O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Job, description:]</td>\n",
       "      <td>[O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4795</th>\n",
       "      <td>[Furthermore, we, expect, you, to, be, able, t...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B, I, I, I, I, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4796</th>\n",
       "      <td>[You, are, structured, and, proactive, and, yo...</td>\n",
       "      <td>[O, O, B, O, B, O, O, O, O, B, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4797</th>\n",
       "      <td>[You, are, a, holistic, and, fact, based, prag...</td>\n",
       "      <td>[O, O, O, B, O, B, I, B, B, I, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4798</th>\n",
       "      <td>[Last, but, not, least, you, both, have, the, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, B, I, I, I, I, I, I, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4799</th>\n",
       "      <td>[Fluency, in, written, and, spoken, English, i...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0     [Senior, QA, Engineer, (, m/f/d, ), <ORGANIZAT...   \n",
       "1     [<ADDRESS>, <ADDRESS>, <ADDRESS>, <ADDRESS>, <...   \n",
       "2                           [Date, posted:, 2021-07-14]   \n",
       "3                   [Likes:, 0, Dislikes:, 0, Love:, 0]   \n",
       "4                                   [Job, description:]   \n",
       "...                                                 ...   \n",
       "4795  [Furthermore, we, expect, you, to, be, able, t...   \n",
       "4796  [You, are, structured, and, proactive, and, yo...   \n",
       "4797  [You, are, a, holistic, and, fact, based, prag...   \n",
       "4798  [Last, but, not, least, you, both, have, the, ...   \n",
       "4799  [Fluency, in, written, and, spoken, English, i...   \n",
       "\n",
       "                                             tags_skill  \n",
       "0                                 [O, O, O, O, O, O, O]  \n",
       "1                                       [O, O, O, O, O]  \n",
       "2                                             [O, O, O]  \n",
       "3                                    [O, O, O, O, O, O]  \n",
       "4                                                [O, O]  \n",
       "...                                                 ...  \n",
       "4795  [O, O, O, O, O, O, O, O, B, I, I, I, I, I, I, ...  \n",
       "4796  [O, O, B, O, B, O, O, O, O, B, O, O, O, O, O, ...  \n",
       "4797  [O, O, O, B, O, B, I, B, B, I, O, O, O, O, O, ...  \n",
       "4798  [O, O, O, O, O, O, O, O, B, I, I, I, I, I, I, ...  \n",
       "4799            [O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "\n",
       "[4800 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55894d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4800 entries, 0 to 4799\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tokens      4800 non-null   object\n",
      " 1   tags_skill  4800 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 75.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4e95c",
   "metadata": {},
   "source": [
    "Protoryping output CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "311cc8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tokens, tags in zip(df_train[\"tokens\"], df_train[\"tags_skill\"]):\n",
    "#     for t, y in zip(tokens, tags):\n",
    "#         print(f\"{t}\\t{y}\")\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef2ad690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of skipped rows: 0\n",
      "Number of skipped rows: 0\n",
      "Invalid BIO sequence: ['O', 'O', 'O', 'O', 'O', 'O', 'I-SKILL', 'I-SKILL', 'I-SKILL']\n",
      "\t\t\tLine: ['Experience', 'with', 'agile', 'approaches', 'to', 'software', 'testing', 'and', 'development']\n",
      "This line will be skipped\n",
      "Number of skipped rows: 1\n",
      "Done. CoNLL files saved to: C:\\Users\\Мариан\\Desktop\\Jupyter Notes\\Projects\\Trainee_iFortex\\Git\\job_posting\\data\\03_validated\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Prepare CoNLL files using only the 'tags_skill' column.\n",
    "# All comments are in English.\n",
    "def normalize_bio_tags(tags, label=\"SKILL\"):\n",
    "    \"\"\"Convert bare BIO like ['O','B','I',...] into typed BIO like ['O','B-SKILL','I-SKILL',...].\"\"\"\n",
    "    out = []\n",
    "    for t in tags:\n",
    "        if t == \"O\":\n",
    "            out.append(\"O\")\n",
    "        elif t == \"B\":\n",
    "            out.append(f\"B-{label}\")\n",
    "        elif t == \"I\":\n",
    "            out.append(f\"I-{label}\")\n",
    "        else:\n",
    "            # already typed or unexpected; keep as is\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def validate_bio_sequence(tags):\n",
    "    \"\"\"\n",
    "    Quick BIO validator: 'I-X' must follow 'B-X' or 'I-X' of the same type.\n",
    "    Returns True if valid.\n",
    "    \"\"\"\n",
    "    prev_type = None\n",
    "    prev_tag = \"O\"\n",
    "    for t in tags:\n",
    "        if t == \"O\":\n",
    "            prev_tag, prev_type = \"O\", None\n",
    "            continue\n",
    "        m = re.match(r\"([BI])-(.+)\", t)\n",
    "        if not m:\n",
    "            return False\n",
    "        bi, lab = m.groups()\n",
    "        if bi == \"B\":\n",
    "            prev_tag, prev_type = \"B\", lab\n",
    "        else:  # I\n",
    "            if prev_tag == \"O\" or prev_type != lab:\n",
    "                return False\n",
    "            prev_tag = \"I\"\n",
    "    return True\n",
    "\n",
    "def write_conll_from_df(df: pd.DataFrame, tokens_col=\"tokens\", tags_col=\"tags_skill\", out_path: Path = Path(\"train.conll\")):\n",
    "    \"\"\"\n",
    "    Write a classic CoNLL file with two columns: token<TAB>label.\n",
    "    Assumes each row has a list of tokens and a same-length list of BIO tags.\n",
    "    \"\"\"\n",
    "    skipped = 0\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for _, row in df.iterrows():\n",
    "            tokens = list(row[tokens_col])\n",
    "            tags   = list(row[tags_col])\n",
    "            assert len(tokens) == len(tags), \"Tokens and tags length mismatch\"\n",
    "            if not validate_bio_sequence(tags):\n",
    "                print(f\"Invalid BIO sequence: {tags}\\n\\t\\t\\tLine: {tokens}\")\n",
    "                print('This line will be skipped')\n",
    "                skipped += 1\n",
    "                continue\n",
    "            for t, y in zip(tokens, tags):\n",
    "                f.write(f\"{t}\\t{y}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    print(f'Number of skipped rows: {skipped}')\n",
    "\n",
    "# === Usage example ===\n",
    "# Suppose you already loaded df_train, df_dev, df_test with columns: \n",
    "#   [\"idx\", \"tokens\", \"tags_skill\", \"tags_knowledge\", \"source\"]\n",
    "\n",
    "for df in (df_train, df_validation, df_test):\n",
    "    df[\"tags_skill\"] = df[\"tags_skill\"].apply(lambda lst: normalize_bio_tags(lst, label=\"SKILL\"))\n",
    "\n",
    "out_dir = config['validated_dir']\n",
    "write_conll_from_df(df_train,      out_path=out_dir / \"train.conll\"     )\n",
    "write_conll_from_df(df_validation, out_path=out_dir / \"validation.conll\")\n",
    "write_conll_from_df(df_test,       out_path=out_dir / \"test.conll\"      )\n",
    "\n",
    "print(\"Done. CoNLL files saved to:\", out_dir.resolve())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a04c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single-file pipeline: CoNLL -> CRF baseline -> BERT token classification (BIO/IOB)\n",
    "# # Requires: pip install sklearn-crfsuite seqeval transformers datasets torch joblib\n",
    "\n",
    "# import os, re, json, random, sys\n",
    "# from pathlib import Path\n",
    "# from typing import List, Tuple, Dict, Any\n",
    "# import joblib\n",
    "\n",
    "# # ---------- CoNLL reader ----------\n",
    "# def read_conll(path: Path) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "#     \"\"\"Read two-column CoNLL (token[TAB]label), blank line separates sentences.\n",
    "#     Returns tokens_list, labels_list where tokens_list[i] aligns to labels_list[i].\"\"\"\n",
    "#     tokens_list, labels_list = [], []\n",
    "#     cur_tokens, cur_labels = [], []\n",
    "#     with path.open(encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.rstrip(\"\\n\")\n",
    "#             if not line:\n",
    "#                 if cur_tokens:\n",
    "#                     tokens_list.append(cur_tokens); labels_list.append(cur_labels)\n",
    "#                     cur_tokens, cur_labels = [], []\n",
    "#                 continue\n",
    "#             parts = line.split(\"\\t\")\n",
    "#             if len(parts) != 2:\n",
    "#                 raise ValueError(f\"Expected 2 columns token<TAB>label, got: {line}\")\n",
    "#             tok, lab = parts\n",
    "#             cur_tokens.append(tok); cur_labels.append(lab)\n",
    "#     if cur_tokens:\n",
    "#         tokens_list.append(cur_tokens); labels_list.append(cur_labels)\n",
    "#     return tokens_list, labels_list\n",
    "\n",
    "# # ---------- Simple BIO features for CRF ----------\n",
    "# def word2features(sent: List[str], i: int) -> Dict[str, Any]:\n",
    "#     \"\"\"Hand-crafted token features for CRF.\"\"\"\n",
    "#     w = sent[i]\n",
    "#     prevw = sent[i-1] if i > 0 else \"__BOS__\"\n",
    "#     nextw = sent[i+1] if i < len(sent)-1 else \"__EOS__\"\n",
    "#     feats = {\n",
    "#         \"bias\": 1.0,\n",
    "#         \"w.lower\": w.lower(),\n",
    "#         \"w.isupper\": w.isupper(),\n",
    "#         \"w.istitle\": w.istitle(),\n",
    "#         \"w.isdigit\": w.isdigit(),\n",
    "#         \"suffix3\": w[-3:],\n",
    "#         \"suffix2\": w[-2:],\n",
    "#         \"prefix2\": w[:2],\n",
    "#         \"prev.lower\": prevw.lower(),\n",
    "#         \"prev.istitle\": prevw.istitle() if prevw not in (\"__BOS__\",\"__EOS__\") else False,\n",
    "#         \"prev.isupper\": prevw.isupper() if prevw not in (\"__BOS__\",\"__EOS__\") else False,\n",
    "#         \"next.lower\": nextw.lower(),\n",
    "#         \"next.istitle\": nextw.istitle() if nextw not in (\"__BOS__\",\"__EOS__\") else False,\n",
    "#         \"next.isupper\": nextw.isupper() if nextw not in (\"__BOS__\",\"__EOS__\") else False,\n",
    "#         \"BOS\": i == 0,\n",
    "#         \"EOS\": i == len(sent)-1,\n",
    "#     }\n",
    "#     return feats\n",
    "\n",
    "# def sent2features(sent: List[str]) -> List[Dict[str, Any]]:\n",
    "#     return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "# # ---------- Span-F1 via seqeval ----------\n",
    "# from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "# def print_seqeval_report(y_true: List[List[str]], y_pred: List[List[str]], title: str):\n",
    "#     print(f\"\\n=== {title} ===\")\n",
    "#     print(\"Precision:\", round(precision_score(y_true, y_pred), 4))\n",
    "#     print(\"Recall   :\", round(recall_score(y_true, y_pred), 4))\n",
    "#     print(\"F1       :\", round(f1_score(y_true, y_pred), 4))\n",
    "#     print(classification_report(y_true, y_pred, digits=4))\n",
    "\n",
    "# # ---------- CRF baseline ----------\n",
    "# def run_crf(train_sents, train_labels, dev_sents, dev_labels, test_sents, test_labels, model_path: Path):\n",
    "#     from sklearn_crfsuite import CRF\n",
    "#     X_train = [sent2features(s) for s in train_sents]\n",
    "#     X_dev   = [sent2features(s) for s in dev_sents]\n",
    "#     X_test  = [sent2features(s) for s in test_sents]\n",
    "#     y_train = train_labels\n",
    "#     y_dev   = dev_labels\n",
    "#     y_test  = test_labels\n",
    "\n",
    "#     crf = CRF(algorithm=\"lbfgs\", c1=0.1, c2=0.1, max_iterations=200, all_possible_transitions=True)\n",
    "#     crf.fit(X_train, y_train)\n",
    "#     y_dev_pred  = crf.predict(X_dev)\n",
    "#     y_test_pred = crf.predict(X_test)\n",
    "\n",
    "#     print_seqeval_report(y_dev, y_dev_pred, \"CRF DEV\")\n",
    "#     print_seqeval_report(y_test, y_test_pred, \"CRF TEST\")\n",
    "\n",
    "#     model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "#     joblib.dump(crf, model_path)\n",
    "#     print(f\"CRF model saved to: {model_path.resolve()}\")\n",
    "\n",
    "# # ---------- BERT token-classification ----------\n",
    "# def run_bert(\n",
    "#     train_sents, train_labels, dev_sents, dev_labels, test_sents, test_labels,\n",
    "#     out_dir: Path, bert_ckpt: str = \"bert-base-cased\", epochs: int = 3, batch_size: int = 16, lr: float = 5e-5\n",
    "# ):\n",
    "#     from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments\n",
    "#     import torch\n",
    "#     from datasets import Dataset, DatasetDict\n",
    "\n",
    "#     # Collect label set preserving BIO strings\n",
    "#     label_list = sorted({lab for seq in (train_labels + dev_labels + test_labels) for lab in seq})\n",
    "#     if any(lab not in {\"O\"} and not re.match(r\"^[BI]-\", lab) for lab in label_list):\n",
    "#         raise ValueError(f\"Labels look non-BIO: {label_list}\")\n",
    "\n",
    "#     label2id = {lab: i for i, lab in enumerate(label_list)}\n",
    "#     id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "#     # Build HF datasets\n",
    "#     def to_rows(sents: List[List[str]], labels: List[List[str]]) -> Dict[str, List[Any]]:\n",
    "#         return {\"tokens\": sents, \"ner_tags\": [[label2id[l] for l in seq] for seq in labels]}\n",
    "\n",
    "#     d_train = Dataset.from_dict(to_rows(train_sents, train_labels))\n",
    "#     d_dev   = Dataset.from_dict(to_rows(dev_sents,   dev_labels))\n",
    "#     d_test  = Dataset.from_dict(to_rows(test_sents,  test_labels))\n",
    "#     ds = DatasetDict({\"train\": d_train, \"validation\": d_dev, \"test\": d_test})\n",
    "\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(bert_ckpt, use_fast=True)\n",
    "\n",
    "#     # Align labels to wordpieces: keep label for first subword, set -100 for others\n",
    "#     def tokenize_and_align(example: Dict[str, Any]) -> Dict[str, Any]:\n",
    "#         toks = example[\"tokens\"]\n",
    "#         labs = example[\"ner_tags\"]\n",
    "#         enc = tokenizer(toks, is_split_into_words=True, truncation=True)\n",
    "#         word_ids = enc.word_ids()\n",
    "#         aligned = []\n",
    "#         prev_word = None\n",
    "#         for idx, wid in enumerate(word_ids):\n",
    "#             if wid is None:\n",
    "#                 aligned.append(-100)\n",
    "#             elif wid != prev_word:\n",
    "#                 aligned.append(labs[wid])\n",
    "#             else:\n",
    "#                 aligned.append(-100)\n",
    "#             prev_word = wid\n",
    "#         enc[\"labels\"] = aligned\n",
    "#         return enc\n",
    "\n",
    "#     ds_tok = ds.map(tokenize_and_align, batched=False)\n",
    "\n",
    "#     model = AutoModelForTokenClassification.from_pretrained(\n",
    "#         bert_ckpt,\n",
    "#         num_labels=len(label_list),\n",
    "#         id2label=id2label,\n",
    "#         label2id=label2id\n",
    "#     )\n",
    "\n",
    "#     data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "#     def compute_metrics(p):\n",
    "#         preds = p.predictions\n",
    "#         if isinstance(preds, tuple):\n",
    "#             preds = preds[0]\n",
    "#         preds = preds.argmax(-1)\n",
    "#         labels = p.label_ids\n",
    "#         # Unpack to BIO strings, skipping -100\n",
    "#         true_str, pred_str = [], []\n",
    "#         for y_true, y_pred in zip(labels, preds):\n",
    "#             y_true_seq, y_pred_seq = [], []\n",
    "#             for t, p_ in zip(y_true, y_pred):\n",
    "#                 if t == -100:\n",
    "#                     continue\n",
    "#                 y_true_seq.append(id2label[int(t)])\n",
    "#                 y_pred_seq.append(id2label[int(p_)])\n",
    "#             true_str.append(y_true_seq)\n",
    "#             pred_str.append(y_pred_seq)\n",
    "#         return {\n",
    "#             \"precision\": precision_score(true_str, pred_str),\n",
    "#             \"recall\":    recall_score(true_str, pred_str),\n",
    "#             \"f1\":        f1_score(true_str, pred_str),\n",
    "#         }\n",
    "\n",
    "#     out_dir.mkdir(parents=True, exist_ok=True)\n",
    "#     args = TrainingArguments(\n",
    "#         output_dir=str(out_dir),\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         learning_rate=lr,\n",
    "#         per_device_train_batch_size=batch_size,\n",
    "#         per_device_eval_batch_size=batch_size,\n",
    "#         num_train_epochs=epochs,\n",
    "#         weight_decay=0.01,\n",
    "#         logging_steps=50,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"f1\",\n",
    "#         report_to=\"none\"\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=args,\n",
    "#         train_dataset=ds_tok[\"train\"],\n",
    "#         eval_dataset=ds_tok[\"validation\"],\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         compute_metrics=compute_metrics\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "#     print(\"\\n=== BERT DEV ===\")\n",
    "#     dev_metrics = trainer.evaluate()\n",
    "#     print({k: round(float(v), 4) for k, v in dev_metrics.items() if k in (\"precision\",\"recall\",\"f1\")})\n",
    "\n",
    "#     print(\"\\n=== BERT TEST ===\")\n",
    "#     test_metrics = trainer.evaluate(ds_tok[\"test\"])\n",
    "#     print({k: round(float(v), 4) for k, v in test_metrics.items() if k in (\"precision\",\"recall\",\"f1\")})\n",
    "\n",
    "#     save_path = out_dir / \"final\"\n",
    "#     trainer.save_model(str(save_path))\n",
    "#     tokenizer.save_pretrained(str(save_path))\n",
    "#     with (out_dir / \"label_mapping.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump({\"label_list\": label_list, \"label2id\": label2id}, f, ensure_ascii=False, indent=2)\n",
    "#     print(f\"BERT model saved to: {save_path.resolve()}\")\n",
    "\n",
    "# # ---------- Entry point ----------\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Configure paths and model\n",
    "#     data_dir = Path(os.environ.get(\"CONLL_DIR\", \"conll_only_skill\"))  # folder with train.conll, validation.conll, test.conll\n",
    "#     train_path = data_dir / \"train.conll\"\n",
    "#     dev_path   = data_dir / \"validation.conll\"\n",
    "#     test_path  = data_dir / \"test.conll\"\n",
    "#     bert_ckpt  = os.environ.get(\"BERT_CKPT\", \"bert-base-cased\")\n",
    "#     out_dir    = Path(os.environ.get(\"BERT_OUT\", \"bert_skill_ner\"))\n",
    "#     random.seed(13)\n",
    "\n",
    "#     train_sents, train_labels = read_conll(train_path)\n",
    "#     dev_sents,   dev_labels   = read_conll(dev_path)\n",
    "#     test_sents,  test_labels  = read_conll(test_path)\n",
    "\n",
    "#     # Quick sanity checks\n",
    "#     assert all(len(s) == len(l) for s, l in zip(train_sents, train_labels)), \"Train tokens/labels len mismatch\"\n",
    "#     assert all(re.match(r\"^(O|[BI]-.+)$\", lab) for seq in train_labels for lab in seq), \"Non-BIO label in train\"\n",
    "\n",
    "#     # CRF baseline\n",
    "#     run_crf(\n",
    "#         train_sents, train_labels,\n",
    "#         dev_sents,   dev_labels,\n",
    "#         test_sents,  test_labels,\n",
    "#         model_path=Path(\"crf_model.joblib\")\n",
    "#     )\n",
    "\n",
    "#     # BERT fine-tuning\n",
    "#     run_bert(\n",
    "#         train_sents, train_labels,\n",
    "#         dev_sents,   dev_labels,\n",
    "#         test_sents,  test_labels,\n",
    "#         out_dir=out_dir,\n",
    "#         bert_ckpt=bert_ckpt,\n",
    "#         epochs=3,\n",
    "#         batch_size=16,\n",
    "#         lr=5e-5\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "245ab068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train sents: 4800  #val: 3174  #test: 3568\n"
     ]
    }
   ],
   "source": [
    "# === 1) Data loading (CoNLL -> lists of sentences) ===\n",
    "def read_conll(path: Path) -> tuple[list[list[str]], list[list[str]]]:\n",
    "    \"\"\"\n",
    "    Read a two-column CoNLL file (token<TAB>tag>), sentences separated by blank lines.\n",
    "    Returns: tokens_per_sent, tags_per_sent as lists of lists.\n",
    "    \"\"\"\n",
    "    s_tokens, s_tags = [], []\n",
    "    tokens, tags = [], []\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    s_tokens.append(tokens)\n",
    "                    s_tags.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "                continue\n",
    "            parts = line.split(\"\\t\")\n",
    "            if len(parts) != 2:\n",
    "                # Skip malformed lines safely\n",
    "                continue\n",
    "            tok, lab = parts\n",
    "            tokens.append(tok)\n",
    "            tags.append(lab)\n",
    "    if tokens:\n",
    "        s_tokens.append(tokens)\n",
    "        s_tags.append(tags)\n",
    "    return s_tokens, s_tags\n",
    "\n",
    "\n",
    "\n",
    "X_tokens_train, y_train = read_conll(config['train_connl'])\n",
    "X_tokens_val,   y_val   = read_conll(config['validation_connl'])\n",
    "X_tokens_test,  y_test  = read_conll(config['test_connl'])\n",
    "\n",
    "\n",
    "print(f\"#train sents: {len(X_tokens_train)}  #val: {len(X_tokens_val)}  #test: {len(X_tokens_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57f0be0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2184, 5944)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_skill_gazetteers(tokens_per_sent: list[list[str]], tags_per_sent: list[list[str]]) -> dict[str, set[str]]:\n",
    "    \"\"\"\n",
    "    Build simple gazetteers from training data:\n",
    "      - skill_unigrams: lowercased tokens appearing inside any SKILL span\n",
    "      - skill_bigrams:  lowercased bigrams inside SKILL spans\n",
    "    \"\"\"\n",
    "    skill_unigrams = set()\n",
    "    skill_bigrams  = set()\n",
    "    for toks, labs in zip(tokens_per_sent, tags_per_sent):\n",
    "        # collect indices of tokens inside SKILL spans (B-SKILL / I-SKILL)\n",
    "        inside = [i for i, t in enumerate(labs) if t.startswith(\"B-\") or t.startswith(\"I-\")]\n",
    "        for i in inside:\n",
    "            skill_unigrams.add(toks[i].lower())\n",
    "        # bigrams (consecutive tokens both inside a span)\n",
    "        for i in range(len(toks) - 1):\n",
    "            if (labs[i].startswith((\"B-\",\"I-\"))) and (labs[i+1].startswith((\"B-\",\"I-\"))):\n",
    "                skill_bigrams.add((toks[i].lower(), toks[i+1].lower()))\n",
    "    return {\"skill_unigrams\": skill_unigrams, \"skill_bigrams\": skill_bigrams}\n",
    "\n",
    "gazetteers = build_skill_gazetteers(X_tokens_train, y_train)\n",
    "len(gazetteers[\"skill_unigrams\"]), len(gazetteers[\"skill_bigrams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a13f2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 400)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def char_ngrams(token: str, n: int) -> list[str]:\n",
    "    token = token\n",
    "    return [token[i:i+n] for i in range(len(token) - n + 1)] if len(token) >= n else []\n",
    "\n",
    "def top_char_ngrams(tokens_per_sent: list[list[str]], top_k: int = 400) -> dict[str, set[str]]:\n",
    "    \"\"\"\n",
    "    Compute most frequent char 2-grams and 3-grams from TRAIN tokens only and keep top_k for each size.\n",
    "    This caps feature explosion while still giving CRF helpful subword signals.\n",
    "    \"\"\"\n",
    "    c2 = Counter()\n",
    "    c3 = Counter()\n",
    "    for sent in tokens_per_sent:\n",
    "        for tok in sent:\n",
    "            c2.update(char_ngrams(tok, 2))\n",
    "            c3.update(char_ngrams(tok, 3))\n",
    "    top2 = set([ng for ng, _ in c2.most_common(top_k)])\n",
    "    top3 = set([ng for ng, _ in c3.most_common(top_k)])\n",
    "    return {\"char2\": top2, \"char3\": top3}\n",
    "\n",
    "char_ngram_vocab = top_char_ngrams(X_tokens_train, top_k=400)\n",
    "len(char_ngram_vocab[\"char2\"]), len(char_ngram_vocab[\"char3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "347e8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREF_SIZES = (2, 3, 4)\n",
    "SUFF_SIZES = (2, 3, 4)\n",
    "\n",
    "def word_shape(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Map token to a coarse 'shape' (e.g., 'Xx', 'xxx', 'd-dd', 'xxx-xx', 'Xx.' etc.).\n",
    "    Helps generalize across casing/digits/punct.\n",
    "    \"\"\"\n",
    "    shape = []\n",
    "    for ch in token:\n",
    "        if ch.isupper():\n",
    "            shape.append('X')\n",
    "        elif ch.islower():\n",
    "            shape.append('x')\n",
    "        elif ch.isdigit():\n",
    "            shape.append('d')\n",
    "        elif ch in \"-_/\\\\.\":\n",
    "            shape.append(ch)\n",
    "        else:\n",
    "            shape.append('p')  # other punct\n",
    "    # collapse runs like XXX -> X, xxx -> x to reduce sparsity\n",
    "    collapsed = []\n",
    "    for ch in shape:\n",
    "        if not collapsed or collapsed[-1] != ch:\n",
    "            collapsed.append(ch)\n",
    "    return ''.join(collapsed)\n",
    "\n",
    "def token_features(sent: list[str], i: int,\n",
    "                   gaz: dict[str, set[str]],\n",
    "                   char_vocab: dict[str, set[str]]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a feature dict for token sent[i].\n",
    "    Includes:\n",
    "      - bias, word lowercase, shape, isupper/istitle/isdigit/has_digit/has_hyphen\n",
    "      - prefixes/suffixes, limited char 2/3-grams (only if in top lists)\n",
    "      - simple gazetteers (unigram + adjacent bigrams), plus +/-1 and +/-2 window features\n",
    "    \"\"\"\n",
    "    token = sent[i]\n",
    "    lower = token.lower()\n",
    "    feats = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower': lower,\n",
    "        'word.shape': word_shape(token),\n",
    "        'word.isupper': token.isupper(),\n",
    "        'word.istitle': token.istitle(),\n",
    "        'word.isdigit': token.isdigit(),\n",
    "        'word.has_digit': any(ch.isdigit() for ch in token),\n",
    "        'word.has_hyphen': '-' in token,\n",
    "        'word.has_dot': '.' in token,\n",
    "        'word.has_slash': '/' in token or '\\\\' in token,\n",
    "        'gaz.in_skill_unigram': (lower in gaz['skill_unigrams']),\n",
    "    }\n",
    "    # prefixes / suffixes\n",
    "    for n in PREF_SIZES:\n",
    "        feats[f'pref{n}'] = lower[:n] if len(lower) >= n else lower\n",
    "    for n in SUFF_SIZES:\n",
    "        feats[f'suff{n}'] = lower[-n:] if len(lower) >= n else lower\n",
    "\n",
    "    # limited char 2/3-grams (only those that are in top vocab to control dimensionality)\n",
    "    for ng in char_ngrams(token, 2):\n",
    "        if ng in char_vocab['char2']:\n",
    "            feats[f'char2={ng}'] = True\n",
    "    for ng in char_ngrams(token, 3):\n",
    "        if ng in char_vocab['char3']:\n",
    "            feats[f'char3={ng}'] = True\n",
    "\n",
    "    # context features (+/- 1, +/- 2)\n",
    "    def add_ctx(j: int, tag: str):\n",
    "        if 0 <= j < len(sent):\n",
    "            w = sent[j]\n",
    "            lw = w.lower()\n",
    "            feats[f'{tag}.lower'] = lw\n",
    "            feats[f'{tag}.shape'] = word_shape(w)\n",
    "            feats[f'{tag}.istitle'] = w.istitle()\n",
    "            feats[f'{tag}.isupper'] = w.isupper()\n",
    "\n",
    "    add_ctx(i-1, '-1')\n",
    "    add_ctx(i-2, '-2')\n",
    "    add_ctx(i+1, '+1')\n",
    "    add_ctx(i+2, '+2')\n",
    "\n",
    "    # gazetteer bigrams with neighbors (prev+cur, cur+next)\n",
    "    if i-1 >= 0:\n",
    "        feats['gaz.prev_cur_in_skill_bigram'] = (sent[i-1].lower(), lower) in gaz['skill_bigrams']\n",
    "    if i+1 < len(sent):\n",
    "        feats['gaz.cur_next_in_skill_bigram'] = (lower, sent[i+1].lower()) in gaz['skill_bigrams']\n",
    "\n",
    "    return feats\n",
    "\n",
    "def sent2features(sent: list[str],\n",
    "                  gaz: dict[str, set[str]],\n",
    "                  char_vocab: dict[str, set[str]]) -> list[dict[str, Any]]:\n",
    "    return [token_features(sent, i, gaz, char_vocab) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd8b048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 4800 3174 3174\n",
      "7 7 {'bias': 1.0, 'word.lower': 'senior', 'word.shape': 'Xx', 'word.isupper': False, 'word.istitle': True, 'word.isdigit': False, 'word.has_digit': False, 'word.has_hyphen': False, 'word.has_dot': False, 'word.has_slash': False, 'gaz.in_skill_unigram': True, 'pref2': 'se', 'pref3': 'sen', 'pref4': 'seni', 'suff2': 'or', 'suff3': 'ior', 'suff4': 'nior', 'char2=Se': True, 'char2=en': True, 'char2=ni': True, 'char2=io': True, 'char2=or': True, '+1.lower': 'qa', '+1.shape': 'X', '+1.istitle': False, '+1.isupper': True, '+2.lower': 'engineer', '+2.shape': 'Xx', '+2.istitle': True, '+2.isupper': False, 'gaz.cur_next_in_skill_bigram': False}\n"
     ]
    }
   ],
   "source": [
    "def to_crf_Xy(tokens_per_sent: list[list[str]],\n",
    "              tags_per_sent: list[list[str]],\n",
    "              gaz: dict[str, set[str]],\n",
    "              char_vocab: dict[str, set[str]]):\n",
    "    X = [sent2features(s, gaz, char_vocab) for s in tokens_per_sent]\n",
    "    y = [list(tags) for tags in tags_per_sent]\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train_ = to_crf_Xy(X_tokens_train, y_train, gazetteers, char_ngram_vocab)\n",
    "X_val_,  y_val_   = to_crf_Xy(X_tokens_val,   y_val,   gazetteers, char_ngram_vocab)\n",
    "X_test_, y_test_  = to_crf_Xy(X_tokens_test,  y_test,  gazetteers, char_ngram_vocab)\n",
    "\n",
    "# Quick sanity check\n",
    "print(len(X_train), len(y_train_), len(X_val_), len(y_val_))\n",
    "print(len(X_train[0]), len(y_train_[0]), X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed900a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRF trained.\n"
     ]
    }
   ],
   "source": [
    "crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,               # L1\n",
    "    c2=0.1,               # L2\n",
    "    max_iterations=200,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "crf.fit(X_train, y_train_)\n",
    "print(\"CRF trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c04f1cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Val [oracle sanity] (span-level) ===\n",
      "Precision: 1.0000  Recall: 1.0000  F1: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SKILL     1.0000    1.0000    1.0000      1070\n",
      "\n",
      "   micro avg     1.0000    1.0000    1.0000      1070\n",
      "   macro avg     1.0000    1.0000    1.0000      1070\n",
      "weighted avg     1.0000    1.0000    1.0000      1070\n",
      "\n",
      "=== Val (span-level) ===\n",
      "Precision: 0.3375  Recall: 0.1000  F1: 0.1543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SKILL     0.3375    0.1000    0.1543      1070\n",
      "\n",
      "   micro avg     0.3375    0.1000    0.1543      1070\n",
      "   macro avg     0.3375    0.1000    0.1543      1070\n",
      "weighted avg     0.3375    0.1000    0.1543      1070\n",
      "\n",
      "=== Test (span-level) ===\n",
      "Precision: 0.3607  Recall: 0.1009  F1: 0.1577\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SKILL     0.3607    0.1009    0.1577      1090\n",
      "\n",
      "   micro avg     0.3607    0.1009    0.1577      1090\n",
      "   macro avg     0.3607    0.1009    0.1577      1090\n",
      "weighted avg     0.3607    0.1009    0.1577      1090\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def eval_seqeval(y_true, y_pred, title: str = \"Eval\"):\n",
    "    print(f\"=== {title} (span-level) ===\")\n",
    "    p = p_span(y_true, y_pred, scheme=IOB2)\n",
    "    r = r_span(y_true, y_pred, scheme=IOB2)\n",
    "    f = f1_span(y_true, y_pred, scheme=IOB2)\n",
    "    print(f\"Precision: {p:.4f}  Recall: {r:.4f}  F1: {f:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, scheme=IOB2, digits=4))\n",
    "\n",
    "y_val_pred  = crf.predict(X_val_)\n",
    "y_test_pred = crf.predict(X_test_)\n",
    "\n",
    "eval_seqeval(y_val_,  y_val_,  title=\"Val [oracle sanity]\")     # sanity: should be 1.0\n",
    "eval_seqeval(y_val_,  y_val_pred,  title=\"Val\")\n",
    "eval_seqeval(y_test_, y_test_pred, title=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c858bb1c",
   "metadata": {},
   "source": [
    "### BERT-CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01aad3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = config['train_connl']\n",
    "val_path   = config['validation_connl']\n",
    "test_path  = config['test_connl']\n",
    "\n",
    "X_train_tok, y_train_bio = read_conll(train_path)\n",
    "X_val_tok,   y_val_bio   = read_conll(val_path)\n",
    "X_test_tok,  y_test_bio  = read_conll(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97cec68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4800 3174 3568\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_tok), len(X_val_tok), len(X_test_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67487e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted({lab for seq in (y_train_bio + y_val_bio + y_test_bio) for lab in seq})\n",
    "assert all(lab == \"O\" or lab.startswith((\"B-\",\"I-\")) for lab in label_list), f\"Non-BIO labels: {label_list}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c059fec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['O', 'B-SKILL', 'I-SKILL'],\n",
       " {'O': 0, 'B-SKILL': 1, 'I-SKILL': 2},\n",
       " {0: 'O', 1: 'B-SKILL', 2: 'I-SKILL'})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = [\"O\"] + [l for l in label_list if l != \"O\"]\n",
    "label2id = {l:i for i,l in enumerate(label_list)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "label_list, label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "01cc52d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_hf(tokens: list[list[str]], labels: list[list[str]]) -> Dataset:\n",
    "    ids = [[label2id[t] for t in seq] for seq in labels]\n",
    "    return Dataset.from_dict({\"tokens\": tokens, \"labels\": ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93d0f5ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'labels'],\n",
       "        num_rows: 4800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'labels'],\n",
       "        num_rows: 3174\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'labels'],\n",
       "        num_rows: 3568\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = DatasetDict({\n",
    "    \"train\": to_hf(X_train_tok, y_train_bio),\n",
    "    \"validation\": to_hf(X_val_tok, y_val_bio),\n",
    "    \"test\": to_hf(X_test_tok, y_test_bio),\n",
    "})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2513f679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4800/4800 [00:00<00:00, 13290.57 examples/s]\n",
      "Map: 100%|██████████| 3174/3174 [00:00<00:00, 18293.64 examples/s]\n",
      "Map: 100%|██████████| 3568/3568 [00:00<00:00, 19127.08 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3174\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3568\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-cased\"  # casing helps for NER\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def tokenize_and_align(batch: dict[str, Any], label_all_tokens: bool = False) -> dict[str, Any]:\n",
    "    enc = tokenizer(\n",
    "        batch[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=256,\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i in range(len(batch[\"tokens\"])):\n",
    "        word_ids = enc.word_ids(batch_index=i)\n",
    "        labels_i = batch[\"labels\"][i]\n",
    "        aligned = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned.append(-100)\n",
    "            elif wid != prev_wid:\n",
    "                aligned.append(labels_i[wid])\n",
    "            else:\n",
    "                aligned.append(labels_i[wid] if label_all_tokens else -100)\n",
    "            prev_wid = wid\n",
    "        all_labels.append(aligned)\n",
    "    enc[\"labels\"] = all_labels\n",
    "    return enc\n",
    "\n",
    "encoded = DatasetDict({\n",
    "    \"train\": ds[\"train\"].map(tokenize_and_align, batched=True, remove_columns=ds[\"train\"].column_names),\n",
    "    \"validation\": ds[\"validation\"].map(tokenize_and_align, batched=True, remove_columns=ds[\"validation\"].column_names),\n",
    "    \"test\": ds[\"test\"].map(tokenize_and_align, batched=True, remove_columns=ds[\"test\"].column_names),\n",
    "})\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "386c7df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ").to(device)\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "def align_for_metrics(predictions: np.ndarray, labels: np.ndarray) -> tuple[list[list[str]], list[list[str]]]:\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    y_true, y_pred = [], []\n",
    "    for p_seq, l_seq in zip(preds, labels):\n",
    "        true_tags, pred_tags = [], []\n",
    "        for p, l in zip(p_seq, l_seq):\n",
    "            if l == -100:\n",
    "                continue\n",
    "            true_tags.append(id2label[int(l)])\n",
    "            pred_tags.append(id2label[int(p)])\n",
    "        y_true.append(true_tags)\n",
    "        y_pred.append(pred_tags)\n",
    "    return y_true, y_pred\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    y_true, y_pred = align_for_metrics(logits, labels)\n",
    "    return {\n",
    "        \"precision\": p_span(y_true, y_pred, scheme=IOB2),\n",
    "        \"recall\":    r_span(y_true, y_pred, scheme=IOB2),\n",
    "        \"f1\":        f1_span(y_true, y_pred, scheme=IOB2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a51537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Мариан\\AppData\\Local\\Temp\\ipykernel_8036\\24730069.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 06:25, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.098400</td>\n",
       "      <td>0.238525</td>\n",
       "      <td>0.430918</td>\n",
       "      <td>0.416822</td>\n",
       "      <td>0.423753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.279275</td>\n",
       "      <td>0.464822</td>\n",
       "      <td>0.549533</td>\n",
       "      <td>0.503640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.315194</td>\n",
       "      <td>0.482173</td>\n",
       "      <td>0.530841</td>\n",
       "      <td>0.505338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.09613342535495759, metrics={'train_runtime': 386.0806, 'train_samples_per_second': 62.163, 'train_steps_per_second': 3.885, 'total_flos': 1428260985022464.0, 'train_loss': 0.09613342535495759, 'epoch': 5.0})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir = str(config['models_dir'] / \"bert-skill-ner\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded[\"train\"],\n",
    "    eval_dataset=encoded[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "616d2c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VALIDATION (span-level, IOB2) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SKILL     0.4822    0.5308    0.5053      1070\n",
      "\n",
      "   micro avg     0.4822    0.5308    0.5053      1070\n",
      "   macro avg     0.4822    0.5308    0.5053      1070\n",
      "weighted avg     0.4822    0.5308    0.5053      1070\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST (span-level, IOB2) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SKILL     0.4896    0.4734    0.4813      1090\n",
      "\n",
      "   micro avg     0.4896    0.4734    0.4813      1090\n",
      "   macro avg     0.4896    0.4734    0.4813      1090\n",
      "weighted avg     0.4896    0.4734    0.4813      1090\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Detailed per-entity report\n",
    "def classification_report_on(split: str):\n",
    "    preds = trainer.predict(encoded[split])\n",
    "    y_true, y_pred = align_for_metrics(preds.predictions, preds.label_ids)\n",
    "    print(f\"\\n=== {split.upper()} (span-level, IOB2) ===\")\n",
    "    print(classification_report(y_true, y_pred, scheme=IOB2, digits=4))\n",
    "\n",
    "classification_report_on(\"validation\")\n",
    "classification_report_on(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f1ab188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- VALIDATION example #20 ---\n",
      "TOKENS: We are in growth mode and looking for high calibre deeply experienced hands-on DevOps engineers who seek to be recognized as platform technology leaders growing our practice .\n",
      "TRUE:   O O O O O O O O B-SKILL I-SKILL O B-SKILL B-SKILL O O O O O O O O O O O O O O O\n",
      "PRED:   O O O O O O O O O O O O B-SKILL O O O O O O O O O O O O O O O\n",
      "TP: [Span(label='SKILL', start=12, end=12)]\n",
      "FP: []\n",
      "FN: [Span(label='SKILL', start=8, end=9), Span(label='SKILL', start=11, end=11)]\n",
      "\n",
      "--- VALIDATION example #21 ---\n",
      "TOKENS: You will work as part of our Agile hybrid DevOps teams to help develop/configure new tools roll out environments and automate processes using a variety of tools and techniques .\n",
      "TRUE:   O O O O O O O O O O O O O B-SKILL I-SKILL I-SKILL B-SKILL I-SKILL I-SKILL O B-SKILL I-SKILL O O O O O O O O\n",
      "PRED:   O O O O O O O O O O O O O B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O B-SKILL I-SKILL O I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O\n",
      "TP: [Span(label='SKILL', start=20, end=21)]\n",
      "FP: [Span(label='SKILL', start=13, end=18)]\n",
      "FN: [Span(label='SKILL', start=13, end=15), Span(label='SKILL', start=16, end=18)]\n",
      "\n",
      "--- VALIDATION example #22 ---\n",
      "TOKENS: This role will give you the opportunity to leverage your current technical skills whilst building your knowledge on the job .\n",
      "TRUE:   O O O O O O O O O O O O O O O O O O O O O\n",
      "PRED:   O O O O O O O O B-SKILL I-SKILL O I-SKILL I-SKILL O B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O\n",
      "TP: []\n",
      "FP: [Span(label='SKILL', start=8, end=9), Span(label='SKILL', start=14, end=19)]\n",
      "FN: []\n",
      "\n",
      "--- VALIDATION example #27 ---\n",
      "TOKENS: Uses testing as a baseline practice: TDD BDD integration E2E\n",
      "TRUE:   B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O O O O\n",
      "PRED:   O O O O O O O O O O\n",
      "TP: []\n",
      "FP: []\n",
      "FN: [Span(label='SKILL', start=0, end=5)]\n",
      "\n",
      "--- VALIDATION example #28 ---\n",
      "TOKENS: Understands the challenges presented by cloud adoption and migration in both enterprise and green field contexts; how to build cloud native applications scratch and how to tackle monolithic system estates through the introduction of API’s microservices and gateways\n",
      "TRUE:   O O O O O O O O O O O O O O O O O O B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O O O B-SKILL I-SKILL I-SKILL I-SKILL O O O O O O O O\n",
      "PRED:   O O O O O O O O O O O O O O O O O O B-SKILL I-SKILL I-SKILL I-SKILL O O O O B-SKILL I-SKILL I-SKILL I-SKILL O O O O O O O O\n",
      "TP: [Span(label='SKILL', start=26, end=29)]\n",
      "FP: [Span(label='SKILL', start=18, end=21)]\n",
      "FN: [Span(label='SKILL', start=18, end=22)]\n",
      "\n",
      "--- VALIDATION example #29 ---\n",
      "TOKENS: Hands on practitioners with the ability to demonstrate and communicate at all levels bringing complex technological issues into perspective for specialists and laymen alike enabling our clients to adapt to changing needs improve their time to live and deliver better solutions though better software .\n",
      "TRUE:   B-SKILL I-SKILL O O O O O O O B-SKILL I-SKILL I-SKILL I-SKILL B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O O O O O B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O O O O O O B-SKILL I-SKILL I-SKILL O O O O\n",
      "PRED:   B-SKILL I-SKILL O O O O O B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O O O O O B-SKILL O O O O O O O B-SKILL O O O O O B-SKILL I-SKILL I-SKILL I-SKILL I-SKILL I-SKILL O\n",
      "TP: [Span(label='SKILL', start=0, end=1), Span(label='SKILL', start=13, end=18)]\n",
      "FP: [Span(label='SKILL', start=7, end=12), Span(label='SKILL', start=24, end=24), Span(label='SKILL', start=32, end=32), Span(label='SKILL', start=38, end=43)]\n",
      "FN: [Span(label='SKILL', start=9, end=12), Span(label='SKILL', start=24, end=31), Span(label='SKILL', start=38, end=40)]\n",
      "\n",
      "--- VALIDATION example #39 ---\n",
      "TOKENS: Consulting experience preferably with major IT consulting or Big 4\n",
      "TRUE:   B-SKILL O O O O O O O O O\n",
      "PRED:   O O O O O O O O O O\n",
      "TP: []\n",
      "FP: []\n",
      "FN: [Span(label='SKILL', start=0, end=0)]\n",
      "\n",
      "--- VALIDATION example #41 ---\n",
      "TOKENS: Highly articulate with good communication skills across diverse groups including stakeholders engineers business analysts and teams\n",
      "TRUE:   O B-SKILL O O B-SKILL I-SKILL O O O O O O O O O O\n",
      "PRED:   O O O O B-SKILL I-SKILL O O O O O O O O O O\n",
      "TP: [Span(label='SKILL', start=4, end=5)]\n",
      "FP: []\n",
      "FN: [Span(label='SKILL', start=1, end=1)]\n"
     ]
    }
   ],
   "source": [
    "from typing import NamedTuple, Set\n",
    "\n",
    "class Span(NamedTuple):\n",
    "    label: str\n",
    "    start: int\n",
    "    end: int  # inclusive\n",
    "\n",
    "def bio_to_spans(tags: list[str]) -> list[Span]:\n",
    "    spans, start, lab = [], None, None\n",
    "    for i, t in enumerate(tags):\n",
    "        if t == \"O\":\n",
    "            if lab is not None:\n",
    "                spans.append(Span(lab, start, i-1))\n",
    "                lab, start = None, None\n",
    "            continue\n",
    "        bi, typ = t.split(\"-\", 1)\n",
    "        if bi == \"B\":\n",
    "            if lab is not None:\n",
    "                spans.append(Span(lab, start, i-1))\n",
    "            lab, start = typ, i\n",
    "        elif bi == \"I\":\n",
    "            pass\n",
    "    if lab is not None:\n",
    "        spans.append(Span(lab, start, len(tags)-1))\n",
    "    return spans\n",
    "\n",
    "def compare_spans(true_spans: list[Span], pred_spans: list[Span]) -> tuple[set[Span], set[Span], set[Span]]:\n",
    "    true_set, pred_set = set(true_spans), set(pred_spans)\n",
    "    return true_set & pred_set, pred_set - true_set, true_set - pred_set\n",
    "\n",
    "def show_errors(split=\"validation\", max_examples=8):\n",
    "    # We need original tokens for display (read them again from your CoNLL-based python lists)\n",
    "    orig_tokens = {\"validation\": X_val_tok, \"test\": X_test_tok, \"train\": X_train_tok}[split]\n",
    "    preds = trainer.predict(encoded[split])\n",
    "    y_true, y_pred = align_for_metrics(preds.predictions, preds.label_ids)\n",
    "    shown = 0\n",
    "    for i, (tseq, pseq) in enumerate(zip(y_true, y_pred)):\n",
    "        tp, fp, fn = compare_spans(bio_to_spans(tseq), bio_to_spans(pseq))\n",
    "        if not fp and not fn:\n",
    "            continue\n",
    "        print(f\"\\n--- {split.upper()} example #{i} ---\")\n",
    "        print(\"TOKENS:\", \" \".join(orig_tokens[i]))\n",
    "        print(\"TRUE:  \", \" \".join(tseq))\n",
    "        print(\"PRED:  \", \" \".join(pseq))\n",
    "        print(\"TP:\", sorted(tp))\n",
    "        print(\"FP:\", sorted(fp))\n",
    "        print(\"FN:\", sorted(fn))\n",
    "        shown += 1\n",
    "        if shown >= max_examples:\n",
    "            break\n",
    "\n",
    "show_errors(\"validation\", max_examples=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job-posting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
